
\section*{Desarrollo}

En 1988 Barzilai y Borwein propusieron dos tamaños de paso para mejorar el desempeño de métodos de descenso de gradiente. La definición de los tamaños de paso se encuentran en la ecuación \ref{eq:barzilai_steps}.

\begin{equation}
	\alpha^{BB1}_k = \frac{s^T_{k-1}s_{k-1}}{s^T_{k-1}y_{k-1}} \qquad \alpha_k^{BB2} = \frac{s_{k-1}^Ty_{k-1}}{y_{k-1}^Ty_{k-1}} \label{eq:barzilai_steps}
\end{equation}

% desigualdad de Cauchy-Schwarz $\|s_{k-1}\|^2\|y_{k-1}\|^2 \geq (s_{k-1}^Ty_{k-1})^2$
donde $s_{k-1} = x_k - x_{k-1}$ y $y_{k-1} = g_k - g_{k-1}$. Considerando la desigualdad de Cauchy-Schwarz observa que cuando $s_{k-1}^Ty_{k-1}$ es mayor a cero, entonces se cumple que $\alpha_k^{BB1} \geq \alpha_k^{BB2}$. Esto por ello, que se suele llamar paso largo e Barzilai-Borwein al paso $\alpha_k^{BB1}$ y paso corto a $\alpha_k^{BB2}$. Para una función cuadrática, el paso $\alpha_k^{BB1}$ es el tamaño de paso de máximo descenso con retardo de un paso y $\alpha_k^{BB2}$ será el paso del método de mínimo gradiente.

El método de Barzilai-Borwein (BB) tiene convergencia R-superlineal para minimizar funciones cuadráticas bidimensionales fuertemente convexas y R-lineal para el caso general n-dimensional\cite{Fletcher_2005}. El método de BB cuenta con la propiedad de reducir los valores de la función objetivo de manera no monóntona. Esta propiedad es una característica intrínseca la cual es la razón de su eficiencia. Sin embargo, es importante para los métodos de gradiente mantener la monotonicidad. Debido a la eficiencia del algoritmo BB y la complejidad de obtener un tamaño de paso $\alpha_k^{SD}$ para el caso general, el trabajo se motiva en la búsqueda de una forma de aceleración para el método de Barzilai-Borwein incorporando pasos monótonos. Considérese el problema de acelerar los métodos de descenso de gradiente que generan secuencias de iterados de la forma

\begin{equation*}
	x_{k+1} = x_k - \alpha_k g_k
\end{equation*}

para resolver el problema

\begin{equation}
	\min_{x\in\mathbb{R}^n}f(x)
\end{equation}

donde $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ es continuamente diferenciable, $g_k=\nabla f(x_k)$ y $\alpha_k>0$ es el tamaño de paso sobre la dirección del gradiente. En particular se usara una función cuadrática (ecuación \ref{eq:quadratic_function}).

\begin{equation}
	f(x) = \frac{1}{2}x^TAx- b^Tx \label{eq:quadratic_function}
\end{equation}

donde $b\in\mathbb{R}^n$ y $A\in \mathbb{R}^{n\times n}$ utilizando el siguiente tamaño de paso mostrado en la ecuación \ref{eq:def_alpha_k_psiA}.

\begin{equation}
	\alpha_k(\Psi(A)) = \frac{g_{k-1}^Tg_{k-1}}{g^T_{k-1}\Psi(A)Ag_{k-1}} \label{eq:def_alpha_k_psiA}
\end{equation}

Donde $\Psi(\cdot)$ es una función real analítica en $[\lambda_1, \lambda_n]$ que se puede expresar como una serie de potencias (ecuación \ref{eq:serie_de_potencias_psi}).

\begin{equation}
	\Psi(z) = \sum\limits_{k=-\infty}^{\infty} c_kz^k,\ c_k\in \mathbb{R}^n  \;\; \text{tal que} \;\; 0<\sum_{k =- \infty}^{\infty}c_k z^k<+\infty \forall z z\in[\lambda_1,  \lambda_n]  \label{eq:serie_de_potencias_psi}
\end{equation}

Los valores de $\lambda_1$ y $\lambda_n$ es el eigenvalor mínimo y máximo respectivamente. Se puede observar que los dos tamaños de paso de Barzilai-Borwein $\alpha_k^{BB1}$ y $\alpha_k^{BB2}$ se pueden calcular a partir de un tamaño de paso más general tomando a $\Psi(A) = I$ y $\Psi(A) = A$ en la ecuación \ref{eq:def_alpha_k_psiA}.

% ------------------------------------------
% Más adelante se procederá a describir el tamaño de paso $\tilde{\alpha}_k$ que se deriva y se utiliza en el trabajo para minimizar funciones cuadráticas bidimensionales convexas en un máximo de 5 iteraciones. También se hablará de modificaciones y consideraciones que se hacen sobre este tamaño de paso para proponer métodos adaptativos no monótonos de gradiente que utilizan los pasos largo y corto del método de BB junto con algunos pasos montonos usando $\tilde{\alpha}_k(A)$. A su vez, se discutirán dos variaciones del método desarrollado para minimizar de forma eficiente funciones más genertales utilizando tamaños de paso con retardo.
% Checar que donde se puede escribir
%--------------------------------------------

\subsection*{Tamaño de paso}

% En esta sección se discutirá sobre el tamaño de paso derivado en el trabajo para minimizar funciones cuadráticas bidimiensionales fuertemente convexas bajo la motivación de requerir una terminación finita.

Se tiene que dos gradientes consecutivos generados por el método de máximo descenso, no se mantienen para métodos que utilizan el tamaño de paso general antes definido. También se observa que el método BB es invariante ante tralaciones y rotaciones cuando se minimizan funciones cuadráticas\cite{dai_2005}, por lo que se puede asumir para simplicidad que la matriz $A$ tiene la forma mostrada en la ecuación \ref{eq:matriz_A}.

\begin{equation}
	A = \text{diag}\{\lambda_1, ..., \lambda_n\}, \qquad \text{donde} \qquad 0<\lambda_1\leq...\leq\lambda_n \label{eq:matriz_A}
\end{equation}


Se ha mostrado que una familia de métodos de gradiente entre los que se incluye máximo descenso y mínimo gradiente reducirán asintóticamente sus búsquedas en un subespacio bidimensional. Se explotaron algunas propiedades ortogonales en este subespacio bidimensional para acelerar los métodos\cite{huang_2022}. %Más adelante se discutirá un caso en el que ocurre que para una función cuadrática, las búsquedas del método de Barzilai-Borwein con tamaño de paso $\alpha_k^{BB1}$ son dominadas en subespacios bidimensionales.
Es por ello que si se quiere acelerar la convergencia de los métodos con tamaño de paso $\alpha_k$ (ecuación \ref{eq:def_alpha_k_psiA}) en un subespacio de menor dimensionalidad se deben mantener propiedades ortogonales. Suponiendo que para un $k>0$, existe un $q_k$ que satisface

\begin{equation*}
	(I-\alpha_{k-1}A)q_k = g_{k-1}
\end{equation*}

donde $q_k$ es invariante ante rotaciones y traslaciones, por lo que se puede asumiendo que la matriz $A$ corresponde a una matriz diagonal de una función cuadrática. Con esto, se tiene el lema \ref{lemma:qk_psi_gk1} que presenta una propiedad para la derivación del tamaño de paso.

\begin{lema}
	Suponiendo que la secuencia $\{g_k\}$ se obtiene aplicando un método de gradiente con pasos como el paso general $\alpha_k)$ para minimizar una función cuadrática y $q_k$ satisface que $(I-\alpha_{k-1}A)q_k = g_{k-1}$. Entonces
	\begin{equation*}
		q_k^T\Psi(A)g_{k+1} = 0.
	\end{equation*}
	\label{lemma:qk_psi_gk1}
\end{lema}

Para probar el lema \ref{lemma:qk_psi_gk1} se tiene que:

\begin{align*}
	q_k^T \Psi(A)g_{k+1} & = q_k^T\Psi(A)(I-\alpha_kA)(I-\alpha_{k-1}A)g_{k-1}                                                        \\
	                     & = q_k^T(I-\alpha_kA)\Psi(A)(I-\alpha_{k-1}A)g_{k-1}                                                        \\
	                     & = g_{k-1}^T\Psi(A)(I-\alpha_kA)g_{k-1}                                                                     \\
	                     & =g_{k-1}^T\Psi(A)g_{k-1}-\alpha_kg_{k-1}^T\Psi(A)Ag_{k-1}                                                  \\
	                     & =g_{k-1}^T\Psi(A)g_{k-1}\left[\frac{g_{k-1}^T\Psi(A)g_{k-1}}{g_{k-1}^T\Psi(A)Ag_{k-1}}-\alpha_k\right] = 0
\end{align*}

con lo que se demuestra que el vector $q_k^T$ y $g_{k+1}$ son ortogonales bajo $\Psi(A)$.

Por el lema \ref{lemma:qk_psi_gk1} se tiene que $g_k^T$  y $q_{k-1}$ son ortogonales bajo $\Psi(A)$ para cualquier $k>0$. Ahora suponemos que los vectores $\Psi^r(A)q_{k-1}$ y $\Psi^{1-r}(A)g_k$ son vectores no nulo, con $r\in\mathbb(R)$. Considerando el problema de minimizar una función $f$ en un subespacio bidimensional generado por

\begin{equation}
	u = \frac{\Psi^r(A)q_{k-1}}{|\Psi^r(A)q_{k-1}|} \qquad
	v = \frac{\Psi^r(A)g_{k}}{|\Psi^r(A)g_{k}|} \label{eq:u_v}
\end{equation}

donde $u$ y $v$ forman una base ortogonal para $\mathbb{R}^2$. Defindiendo una función $\phi$ como

\begin{equation}
	\phi(t,l):=f\left(x_k+t\frac{\Psi^r(A)q_{k-1}}{\|\Psi^r(A)q_{k-1}\|} + l \frac{\Psi^r(A)g_{k}}{\|\Psi^r(A)g_{k}\|}\right) \label{eq:phi_definition}
\end{equation}

al expandir en una serie de Taylor obtenemos que

\begin{equation}
	\phi(t,l) = f(x_k)+\nabla^T f(x_k) \left[tu+lv \right] + \frac{1}{2}\left[tu+ lv \right]^T\nabla^2 f(x_k) \left[tu+lv \right] \label{eq:phi_taylor}
\end{equation}

Tomando a una matriz $B_k$ como en la ecuación \ref{eq:matrix_bk}.

\begin{equation}
	B_k =  \left(u, v\right)^T \label{eq:matrix_bk}
\end{equation}

Con las ecuaciones \ref{eq:u_v} y \ref{eq:matrix_bk} podemos escribir la ecuación \ref{eq:phi_taylor} como en la ecuación \ref{eq:phi_u_v_bk}.

\begin{equation}
	\phi(t, l) =  f(x_k)+g_k^TB_k^T\begin{pmatrix}t\\l\end{pmatrix} +\frac{1}{2}\begin{pmatrix}t\\l\end{pmatrix}^TB_k A B_k^T\begin{pmatrix}t\\l\end{pmatrix} \label{eq:phi_u_v_bk}
\end{equation}

Definiendo el vector $\vartheta$ como

\begin{equation}
	\vartheta_k = B_kg_k =
	\begin{pmatrix}
		\frac{g_k^T\Psi^r(A)q_{k-1}}{|\Psi^{r}(A)q_{k-1}|} \\[0.25cm]
		\frac{g_k^T\Psi^{1-r}(A)g_k}{|\Psi^{1-r}(A)g_k|}
	\end{pmatrix}
	\label{eq:vartheta_k}
\end{equation}

y al hessiano de $f$ en el paso $k$ como

\begin{equation}
	H_k = B_kAB_k^T =\begin{pmatrix}
		\frac{q_{k-1}^T\Psi^{2r}(A)Aq_{k-1}}{\|\Psi^{r}(A)q_{k-1}\|^2}             & \frac{q_{k-1}^T\Psi^r(A)Ag_{k}}{\|\Psi^{r}(A)q_{k-1}\|\Psi^{1-r}(A)g_k\| } \\
		\frac{q_{k-1}^T\Psi^r(A)Ag_{k}}{\|\Psi^{r}(A)q_{k-1}\|\Psi^{1-r}(A)g_k\| } &
		\frac{g_k^T\Psi^{2(1-r)}(A)Ag{k}}{\|\Psi^{r}(A)g_k\|^2}
	\end{pmatrix} \label{eq:hk_bkabk}
\end{equation}

Entonces la función $f\phi(t, l)$ se puede escribir como en la ecuación \ref{eq:phi_with_vectors}.

\begin{equation}
	\phi(t, l) =  f(x_k)+\vartheta_k^T
	\begin{pmatrix}
		t \\l
	\end{pmatrix} +\frac{1}{2}
	\begin{pmatrix}
		t \\l
	\end{pmatrix}^T
	H_k
	\begin{pmatrix}
		t \\l
	\end{pmatrix} \label{eq:phi_with_vectors}
\end{equation}


Denotando las componentes de $H_k$ por $H_k^(ij)$, $i,j = 1,2$ y notando que $B_kB^T_k = I$ se tiene el teorema \ref{teo:alpha_tilde}.

\begin{teor}[Terminación finita]

	Suponga que un método de gradiente se aplica para minimizar una función cuadrá\-tica bidimensional con $\alpha_k$ dado por el paso general mencionado anteriormente para todas las $k\neq k_0$ y usa el tamaño de paso

	\begin{equation*}
		\tilde{\alpha}_{k_0} = \frac{2}{\left(H^{(11)}_{k_0} + H^{(22)}_{k_0}\right)+\sqrt{\left(H^{(11)}_{k_0} - H^{(22)}_{k_0}\right)^2 } + 4\left(H_{k_0}^{(12)}\right)^2}
	\end{equation*}

	en la iteración $k_0$-ésima donde $k_0\geq 2$. Entonces, el método encontrará el minimizador en máximo $k_0+3$ iteraciones.
	\label{teo:alpha_tilde}
\end{teor}


Para demostrar el teorema \ref{teo:alpha_tilde} suponemos que $x_k$ no es un minimizador para toda $k=1, ..., k_0+1$. Para simplificar durante la prueba, se utilizará comom notación $k$ para referirse a $k_0$.

Es necesario observar que $\tilde{\alpha_k}$ satisface la ecuación \ref{eq:alpha_tilde}.

\begin{equation}
	\tilde{\alpha}^2_k\Delta-\tilde{\alpha}_k\left(H^{(11)}_k + H_k^{(22)}\right) + 1 = 0
	\label{eq:alpha_tilde}
\end{equation}

en donde $\Delta = \det{(H_k)}$ = $\det{(A)}>0$. Ahora, sea

\begin{equation*}
	\Theta = \left(H_k^{(12)}\vartheta_k^{(1)}+ H_k^{(22)}\vartheta^{(2)}_k\right)\vartheta_k^{(1)} -  \left(H_k^{(11)}\vartheta_k^{(1)}+ H_k^{(12)}\vartheta^{(2)}_k\right)\vartheta_k^{(2)}
\end{equation*}

en donde $\vartheta_k^{(i)}$ son las componentes de $\vartheta_k$. Multiplicando $\Theta$ a la ecuación \ref{eq:alpha_tilde} se tiene lo siguiente

\begin{equation*}
	\tilde{\alpha}_k^2\Delta\Theta-\tilde{\alpha_k}\left(H_k^{(11)}+H_k^{(12)}\right)\Theta + \Theta = 0,
\end{equation*}

que exactamente es  CHECAR

\begin{align*}
	(H_k^{(22)}v_k^{1}-H_k^{(12)}v_k^{(2)}-\tilde{\alpha}_k\Delta\vartheta_k^{(1)})[\vartheta_k ^{(2)}-\tilde{\alpha}_k(H_k^{(12)}\vartheta_k^{(1)}+H_k^{(22)}\vartheta_k^{(2)})] \\
	= (H_k^{(11)}v_k^{2}-H_k^{(12)}v_k^{(1)}-\tilde{\alpha}_k\Delta\vartheta_k^{(1)})[\vartheta_k ^{(1)}-\tilde{\alpha}_k(H_k^{(11)}\vartheta_k^{(1)}+H_k^{(12)}\vartheta_k^{(2)})].
\end{align*}

Consideremos dos vectores $\boldsymbol{A} = (a_1, a_2)^T$ y $\boldsymbol{B} = (b_1, b_2)^T$, tales que se cumple $a_1b_2 = a_2b_1$. Sustituyendo $b_2$ en $\boldsymbol{B}$, tenemos lo siguiente

\begin{equation*}
	\boldsymbol{B} = \begin{pmatrix}
		b_1 \\
		\frac{b_1a_2}{a_1}
	\end{pmatrix} = \frac{b_1}{a_1}\boldsymbol{A}
\end{equation*}

por los que podemos decir que $\boldsymbol{B}$ es paralelo a $\boldsymbol{A}$. Haciendo uso del resultado anterior podemos decir que el vector

\begin{equation*}
	\begin{pmatrix}
		H_k^{(22)}\vartheta_k^{(1)}-H_k^{(12)}\vartheta_k^{(2)}-\tilde{\alpha}_k\Delta\vartheta_k^{(1)} \\[0.25cm]
		H_k^{(11)}\vartheta_k^{(2)}-H_k^{(12)}\vartheta_k^{(1)}-\tilde{\alpha}_k\Delta\vartheta_k^{(2)}
	\end{pmatrix}
\end{equation*}

es paralelo a

\begin{equation*}
	\begin{pmatrix}
		v_k^{(1)} - \tilde{\alpha}_k(H_k^{(11)}\vartheta_k^{(1)}+H_k^{(12)}\vartheta_k^{(2)}) \\[0.25cm]
		v_k^{(2)} - \tilde{\alpha}_k(H_k^{(12)}\vartheta_k^{(1)}+H_k^{(22)}\vartheta_k^{(2)})
	\end{pmatrix}
\end{equation*}

Por ende el vector $\vartheta_k+H_k(-\tilde{\alpha}_k\vartheta_k)$ es paralelo a  $H_k^{-1}\vartheta_j-\tilde{\alpha}_k\vartheta_k$. Es decir

\begin{equation}
	\vartheta_k+H_k(-\tilde{\alpha}_k\vartheta_k) = \gamma(H_k^{-1}\vartheta_j-\tilde{\alpha}_k\vartheta_k) \label{eq:vartheta_hk}
\end{equation}

donde $\gamma\neq 0  \in \mathbb{R}$. Si multiplicamos por la derecha a la ecuación \ref{eq:vartheta_hk} por $B_k^T$ se tiene que

\begin{equation*}
	B_k^T[\vartheta_k+H_k(-\tilde{\alpha}_k\vartheta_k)] = \gamma B_k^T(H_k^{-1}\vartheta_j-\tilde{\alpha}_k\vartheta_k).
\end{equation*}

Se sabe que $B^T_kB_k = I$, $\vartheta_k = B_kg_k$ y que $H_k = B_kAB_k^T$. Además $g_{k+1} = g_k+\tilde{\alpha}_kAg_k$. Haciendo uso de este conocimiento, se obtiene que

\begin{align*}
	g_k+A(-\tilde{\alpha}_kB^T_k\vartheta_k) & = \gamma(B_k^TH_k^{-1}\vartheta_k+\tilde{\alpha}_kg_k)       \\
	g_k+A(-\tilde{\alpha}_kg_k)              & = \gamma(B_k^TH_k^{-1}\vartheta_k+\tilde{\alpha}_kg_k)       \\
	g_{k+1}                                  & = \gamma(B_k^TH_k^{-1}\vartheta_k+\tilde{\alpha}_kg_k)       \\
	g_{k+1}                                  & = \gamma(B_k^TB_kA^{-1}B_k^T\vartheta_k+\tilde{\alpha}_kg_k) \\
	g_{k+1}                                  & = \gamma(A^{-1}g_k+\tilde{\alpha}_kg_k)
\end{align*}

Factorizando $A^{-1}$ se tiene

\begin{equation*}
	g_{k+1} = \gamma A^{-1}(g_k+\tilde{\alpha}_kAg_k)
\end{equation*}

Considerando que $g_{k+1} = g_k+\tilde{\alpha}_kAg_k$ llegamos a

\begin{equation*}
	g_{k+1} = \gamma A^{-1}g_{k+1}
\end{equation*}

Es decir $g_{k+1}$ es un eigenvector de la matriz $A$. Por hipótesis, sabemos que $x_{k+2}$ no es un minimizador, así que $g_{k+2}\neq 0$ y el algoritmo no se detendrá en la $k+2$-ésima iteración. Entonces, calculando $\alpha_{k+2}$ como

\begin{equation*}
	\alpha_{k+2}= \frac{g_{k+1}^T\Psi(A)g_{k+1}}{g_{k+1}^T\Psi(A)Ag_{k+1}} = 1/\lambda
\end{equation*}

tenemos que

\begin{align*}
	g_{k+3} & = (I-\alpha_{k+2}A)g_{k+2}         \\
	        & = (1-\alpha_{k+2}\lambda)g_{k+2}=0
\end{align*}

lo que implica que $x_{k+3}$ debe ser el minimizador. Si tomamos $k_0 =2$ en el teorema, el tamaño de paso encontrará al minimizador exacto en máximo 5 iteraciones cuando se tiene una función cuadrática bidimensional fuertemente convexa.
